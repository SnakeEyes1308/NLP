{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Content\n",
    "1. [Introduction](#1.-Introduction)<br>\n",
    "    1.1. [Author's Details](#1.1.-Author's-Details)<br>\n",
    "    1.2. [Downloading and Importing the required Libraries](#1.2.-Downloading-and-Importing-the-required-Libraries)<br>\n",
    "    1.3. [Loading the Dataset](#1.3.-Loading-the-Dataset)<br>\n",
    "    1.4. [Splitting the Dataset into Train and Test](#1.4.-Splitting-the-Dataset-into-Train-and-Test)<br>\n",
    "    1.5. [Describing the parameters for the model](#1.5.-Describing-the-parameters-for-the-model)<br>\n",
    "2. [Tokenization](#2-Tokenization)<br>\n",
    "    2.1. [Importing the Tokenizer](#2.1.-Importing-the-Tokenizer)<br>\n",
    "    2.2. [Adding the summarization prefix](#2.2.-Adding-the-summarization-prefix)<br>\n",
    "3. [Preprocessing](#3.-Preprocessing)<br>\n",
    "    3.1. [Defining the Preprocessing Function](#3.1.-Defining-the-Preprocessing-Function)<br>\n",
    "    3.2. [Mapping the Preprocessing Function to the Train and Test Datasets](#3.2.-Mapping-the-Preprocessing-Function-to-the-Train-and-Test-Datasets)<br>\n",
    "4. [Training](#4.-Training)<br>\n",
    "    4.1. [Improting the Tensorflow Model](#4.1.-Improting-the-Tensorflow-Model)<br>\n",
    "    4.2. [Using the DataCollatorForSeq2Seq for collating the data](#4.2.-Using-the-DataCollatorForSeq2Seq-for-collating-the-data)<br>\n",
    "    4.3. [Creating the Training, Test and Generation Datasets](#4.3.-Creating-the-Training,-Test-and-Generation-Datasets)<br>\n",
    "5. [Fine-Tuning](#5.-Fine-Tuning)<br>\n",
    "    5.1. [Tuning optimizer and compiling the model](#5.1.-Tuning-optimizer-and-compiling-the-model)<br>\n",
    "    5.2. [Model Summary](#5.2.-Model-Summary)<br>\n",
    "6. [Evaluation](#6.-Evaluation)<br>\n",
    "    6.1. [Importing the RougeL Metric](#6.1.-Importing-the-RougeL-Metric)<br>\n",
    "7. [Training the Model](#7.-Training-the-Model)<br>\n",
    "    7.1. [Training the Model](#7.1.-Training-the-Model)<br>\n",
    "8. [Testing the Model](#8.-Testing-the-Model)<br>\n",
    "    8.1. [Testing the Model](#8.1.-Testing-the-Model)<br>\n",
    "9. [Saving the Model](#9.-Saving-the-Model)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.1. Author's Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Name : Nisarg Patel <br>\n",
    "## PRN : 21070126060 <br>\n",
    "## Batch : AIML - A3 <br>\n",
    "## Git-Repo : [GitRepo](https://github.com/SnakeEyes1308/NLP) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.2. Downloading and Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:08:25.102834Z",
     "iopub.status.busy": "2023-09-27T13:08:25.101713Z",
     "iopub.status.idle": "2023-09-27T13:09:18.233022Z",
     "shell.execute_reply": "2023-09-27T13:09:18.231849Z",
     "shell.execute_reply.started": "2023-09-27T13:08:25.102777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q\n",
    "!pip install keras_nlp -q\n",
    "!pip install datasets -q\n",
    "!pip install huggingface-hub -q\n",
    "!pip install rouge-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:09:18.235792Z",
     "iopub.status.busy": "2023-09-27T13:09:18.235438Z",
     "iopub.status.idle": "2023-09-27T13:10:00.037578Z",
     "shell.execute_reply": "2023-09-27T13:10:00.036555Z",
     "shell.execute_reply.started": "2023-09-27T13:09:18.235750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0927 13:09:51.240667260      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0927 13:09:51.240691774      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0927 13:09:51.240695346      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0927 13:09:51.240698246      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0927 13:09:51.240700761      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0927 13:09:51.240703686      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0927 13:09:51.240706300      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0927 13:09:51.240709794      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0927 13:09:51.240712399      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0927 13:09:51.240714981      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0927 13:09:51.240717523      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0927 13:09:51.240720131      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0927 13:09:51.240722819      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0927 13:09:51.240725359      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0927 13:09:51.240926792      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\n",
      "D0927 13:09:51.246311038      15 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0927 13:09:51.246341979      15 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0927 13:09:51.246741129      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0927 13:09:51.246753005      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0927 13:09:51.246757156      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0927 13:09:51.246760808      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0927 13:09:51.246764443      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0927 13:09:51.246768013      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0927 13:09:51.246776060      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0927 13:09:51.246795623      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0927 13:09:51.246836567      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0927 13:09:51.246853547      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0927 13:09:51.246858420      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0927 13:09:51.246862987      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0927 13:09:51.246875434      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0927 13:09:51.246879674      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0927 13:09:51.246883730      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0927 13:09:51.246888797      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0927 13:09:51.249397944      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0927 13:09:51.265229770     397 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0927 13:09:51.272197672     397 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-09-27T13:09:51.272182351+00:00\", grpc_status:2}\n"
     ]
    }
   ],
   "source": [
    "# Importing the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.3. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:10:06.206558Z",
     "iopub.status.busy": "2023-09-27T13:10:06.205930Z",
     "iopub.status.idle": "2023-09-27T13:12:15.927373Z",
     "shell.execute_reply": "2023-09-27T13:12:15.926475Z",
     "shell.execute_reply.started": "2023-09-27T13:10:06.206522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.76k/5.76k [00:00<00:00, 10.5MB/s]\n",
      "Downloading readme: 100%|██████████| 6.24k/6.24k [00:00<00:00, 20.0MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|          | 1.57M/255M [00:00<00:16, 15.7MB/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 6.34M/255M [00:00<00:07, 34.5MB/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 11.3M/255M [00:00<00:05, 41.3MB/s]\u001b[A\n",
      "Downloading data:   6%|▋         | 16.2M/255M [00:00<00:05, 44.3MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 21.2M/255M [00:00<00:05, 46.3MB/s]\u001b[A\n",
      "Downloading data:  10%|█         | 26.1M/255M [00:00<00:04, 47.4MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 31.1M/255M [00:00<00:04, 48.2MB/s]\u001b[A\n",
      "Downloading data:  14%|█▍        | 36.0M/255M [00:00<00:04, 48.6MB/s]\u001b[A\n",
      "Downloading data:  16%|█▌        | 41.0M/255M [00:00<00:04, 48.9MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 46.0M/255M [00:01<00:04, 49.1MB/s]\u001b[A\n",
      "Downloading data:  20%|██        | 51.0M/255M [00:01<00:04, 49.4MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 55.9M/255M [00:01<00:04, 49.4MB/s]\u001b[A\n",
      "Downloading data:  24%|██▍       | 60.9M/255M [00:01<00:03, 49.6MB/s]\u001b[A\n",
      "Downloading data:  26%|██▌       | 65.9M/255M [00:01<00:03, 49.7MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 70.9M/255M [00:01<00:03, 49.8MB/s]\u001b[A\n",
      "Downloading data:  30%|██▉       | 75.9M/255M [00:01<00:03, 49.9MB/s]\u001b[A\n",
      "Downloading data:  32%|███▏      | 81.0M/255M [00:01<00:03, 50.1MB/s]\u001b[A\n",
      "Downloading data:  34%|███▍      | 86.0M/255M [00:01<00:03, 50.2MB/s]\u001b[A\n",
      "Downloading data:  36%|███▌      | 91.1M/255M [00:01<00:03, 50.3MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 96.1M/255M [00:02<00:03, 50.4MB/s]\u001b[A\n",
      "Downloading data:  40%|███▉      | 101M/255M [00:02<00:03, 50.5MB/s] \u001b[A\n",
      "Downloading data:  42%|████▏     | 106M/255M [00:02<00:02, 50.5MB/s]\u001b[A\n",
      "Downloading data:  44%|████▎     | 111M/255M [00:02<00:02, 50.5MB/s]\u001b[A\n",
      "Downloading data:  46%|████▌     | 116M/255M [00:02<00:02, 50.3MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 121M/255M [00:02<00:02, 50.3MB/s]\u001b[A\n",
      "Downloading data:  50%|████▉     | 126M/255M [00:02<00:02, 50.1MB/s]\u001b[A\n",
      "Downloading data:  52%|█████▏    | 131M/255M [00:02<00:02, 50.0MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▎    | 136M/255M [00:02<00:02, 49.9MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▌    | 141M/255M [00:02<00:02, 49.4MB/s]\u001b[A\n",
      "Downloading data:  57%|█████▋    | 146M/255M [00:03<00:02, 49.1MB/s]\u001b[A\n",
      "Downloading data:  59%|█████▉    | 151M/255M [00:03<00:02, 49.3MB/s]\u001b[A\n",
      "Downloading data:  61%|██████▏   | 156M/255M [00:03<00:02, 49.1MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 161M/255M [00:03<00:01, 48.8MB/s]\u001b[A\n",
      "Downloading data:  65%|██████▌   | 166M/255M [00:03<00:01, 49.0MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 171M/255M [00:03<00:01, 49.2MB/s]\u001b[A\n",
      "Downloading data:  69%|██████▉   | 176M/255M [00:03<00:01, 49.2MB/s]\u001b[A\n",
      "Downloading data:  71%|███████   | 181M/255M [00:03<00:01, 49.4MB/s]\u001b[A\n",
      "Downloading data:  73%|███████▎  | 186M/255M [00:03<00:01, 48.8MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▍  | 191M/255M [00:03<00:01, 48.6MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 196M/255M [00:04<00:01, 48.3MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 201M/255M [00:04<00:01, 48.3MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 205M/255M [00:04<00:01, 48.6MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 210M/255M [00:04<00:00, 48.7MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▍ | 215M/255M [00:04<00:00, 47.8MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▋ | 220M/255M [00:04<00:00, 47.3MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 225M/255M [00:04<00:00, 47.5MB/s]\u001b[A\n",
      "Downloading data:  90%|█████████ | 230M/255M [00:04<00:00, 46.9MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 234M/255M [00:04<00:00, 45.3MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▍| 239M/255M [00:04<00:00, 44.5MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████▌| 243M/255M [00:05<00:00, 43.8MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 248M/255M [00:05<00:00, 43.4MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 255M/255M [00:05<00:00, 47.6MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]\n",
      "Downloading data: 2.72MB [00:00, 36.5MB/s]                   \u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:06<00:00,  3.36s/it]\n",
      "Generating train split: 100%|██████████| 204045/204045 [00:57<00:00, 3519.53 examples/s]\n",
      "Generating validation split: 100%|██████████| 11332/11332 [00:31<00:00, 356.35 examples/s]\n",
      "Generating test split: 100%|██████████| 11334/11334 [00:31<00:00, 355.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary', 'id'],\n",
      "    num_rows: 204045\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the Xsum Dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"xsum\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:12:15.929354Z",
     "iopub.status.busy": "2023-09-27T13:12:15.928977Z",
     "iopub.status.idle": "2023-09-27T13:12:15.934456Z",
     "shell.execute_reply": "2023-09-27T13:12:15.933675Z",
     "shell.execute_reply.started": "2023-09-27T13:12:15.929293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.', 'id': '35232142'}\n"
     ]
    }
   ],
   "source": [
    "# Printing the first element of the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.4. Splitting the Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:12:15.935692Z",
     "iopub.status.busy": "2023-09-27T13:12:15.935421Z",
     "iopub.status.idle": "2023-09-27T13:12:15.976684Z",
     "shell.execute_reply": "2023-09-27T13:12:15.975914Z",
     "shell.execute_reply.started": "2023-09-27T13:12:15.935667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test\n",
    "datasets = dataset.train_test_split(train_size=0.1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:12:38.996346Z",
     "iopub.status.busy": "2023-09-27T13:12:38.995849Z",
     "iopub.status.idle": "2023-09-27T13:12:39.001115Z",
     "shell.execute_reply": "2023-09-27T13:12:39.000327Z",
     "shell.execute_reply.started": "2023-09-27T13:12:38.996306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20404\n",
      "20405\n"
     ]
    }
   ],
   "source": [
    "# Checking the length of the train and test split\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.5. Describing the parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:12:48.929407Z",
     "iopub.status.busy": "2023-09-27T13:12:48.929040Z",
     "iopub.status.idle": "2023-09-27T13:12:48.934514Z",
     "shell.execute_reply": "2023-09-27T13:12:48.933626Z",
     "shell.execute_reply.started": "2023-09-27T13:12:48.929375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define maximum input length for tokenization\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "\n",
    "# Define minimum and maximum target lengths for summarization\n",
    "MIN_TARGET_LENGTH = 5\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# Define batch size for training\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Define learning rate for optimizer\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Define maximum number of epochs for training\n",
    "MAX_EPOCHS = 1\n",
    "\n",
    "# Define the pre-trained T5 model checkpoint to be used\n",
    "MODEL_CHECKPOINT = \"t5-small\" # Name of Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2.1. Importing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:12:58.018600Z",
     "iopub.status.busy": "2023-09-27T13:12:58.017726Z",
     "iopub.status.idle": "2023-09-27T13:12:58.847116Z",
     "shell.execute_reply": "2023-09-27T13:12:58.846204Z",
     "shell.execute_reply.started": "2023-09-27T13:12:58.018555Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 1.38MB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 12.0MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 25.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Importing the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2.2. Adding the summarization prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:13:16.154543Z",
     "iopub.status.busy": "2023-09-27T13:13:16.154148Z",
     "iopub.status.idle": "2023-09-27T13:13:16.159522Z",
     "shell.execute_reply": "2023-09-27T13:13:16.158599Z",
     "shell.execute_reply.started": "2023-09-27T13:13:16.154510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the specified model checkpoint is either \"t5-small\" or \"t5-base\"\n",
    "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\"]:\n",
    "    # If yes, set the prefix for summarization task\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    # If not, set an empty prefix\n",
    "    prefix = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3.1. Defining the Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:13:25.558719Z",
     "iopub.status.busy": "2023-09-27T13:13:25.558356Z",
     "iopub.status.idle": "2023-09-27T13:13:25.564798Z",
     "shell.execute_reply": "2023-09-27T13:13:25.563999Z",
     "shell.execute_reply.started": "2023-09-27T13:13:25.558691Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3.2. Mapping the Preprocessing Function to the Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:13:34.438943Z",
     "iopub.status.busy": "2023-09-27T13:13:34.438585Z",
     "iopub.status.idle": "2023-09-27T13:13:59.743912Z",
     "shell.execute_reply": "2023-09-27T13:13:59.742826Z",
     "shell.execute_reply.started": "2023-09-27T13:13:34.438913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/20404 [00:00<?, ? examples/s]/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 20404/20404 [00:12<00:00, 1614.67 examples/s]\n",
      "Map: 100%|██████████| 20405/20405 [00:12<00:00, 1615.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping the preprocessing function to the train and test datasets\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4.1. Improting the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:14:15.736830Z",
     "iopub.status.busy": "2023-09-27T13:14:15.736399Z",
     "iopub.status.idle": "2023-09-27T13:14:47.282896Z",
     "shell.execute_reply": "2023-09-27T13:14:47.281646Z",
     "shell.execute_reply.started": "2023-09-27T13:14:15.736798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 166kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 242M/242M [00:05<00:00, 44.2MB/s] \n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Using tensorflow AutoModelForSeq2SeqLM for summarization\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4.2. Using the DataCollatorForSeq2Seq for collating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:14:50.889715Z",
     "iopub.status.busy": "2023-09-27T13:14:50.888515Z",
     "iopub.status.idle": "2023-09-27T13:14:50.900369Z",
     "shell.execute_reply": "2023-09-27T13:14:50.899305Z",
     "shell.execute_reply.started": "2023-09-27T13:14:50.889679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the DataCollatorForSeq2Seq for collating the data\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model,return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4.3. Creating the Training, Test and Generation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:15:04.599501Z",
     "iopub.status.busy": "2023-09-27T13:15:04.599020Z",
     "iopub.status.idle": "2023-09-27T13:15:05.560306Z",
     "shell.execute_reply": "2023-09-27T13:15:05.558897Z",
     "shell.execute_reply.started": "2023-09-27T13:15:04.599469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Create a training dataset from the tokenized training data\n",
    "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,                 # Set the batch size for training data\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],  # Specify columns to include in the dataset\n",
    "    shuffle=True,                          # Shuffle the data for randomness\n",
    "    collate_fn=data_collator               # Use the specified data collator function\n",
    ")\n",
    "\n",
    "# Create a test dataset from the tokenized test data\n",
    "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,                 # Set the batch size for test data\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],  # Specify columns to include in the dataset\n",
    "    shuffle=False,                         # Do not shuffle the test data\n",
    "    collate_fn=data_collator               # Use the specified data collator function\n",
    ")\n",
    "\n",
    "# Create a generation dataset by selecting a subset of data from the tokenized test data\n",
    "generation_dataset = (\n",
    "    tokenized_datasets[\"test\"]             # Access the tokenized test data\n",
    "    .shuffle()                             # Shuffle the data for randomness\n",
    "    .select(list(range(200)))              # Select a specific range of samples\n",
    "    .to_tf_dataset(                        # Convert the selected data to a TF dataset\n",
    "        batch_size=BATCH_SIZE,             # Set the batch size\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],  # Specify columns to include\n",
    "        shuffle=False,                     # Do not shuffle the generation data\n",
    "        collate_fn=data_collator           # Use the specified data collator function\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 5. Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 5.1. Tuning optimizer and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:15:17.330955Z",
     "iopub.status.busy": "2023-09-27T13:15:17.329985Z",
     "iopub.status.idle": "2023-09-27T13:15:17.352369Z",
     "shell.execute_reply": "2023-09-27T13:15:17.351271Z",
     "shell.execute_reply.started": "2023-09-27T13:15:17.330909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tuning the optimizer and compiling the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 5.2. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:15:25.066653Z",
     "iopub.status.busy": "2023-09-27T13:15:25.065942Z",
     "iopub.status.idle": "2023-09-27T13:15:25.100870Z",
     "shell.execute_reply": "2023-09-27T13:15:25.099736Z",
     "shell.execute_reply.started": "2023-09-27T13:15:25.066617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 6.1. Importing the RougeL Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T13:15:36.407991Z",
     "iopub.status.busy": "2023-09-27T13:15:36.406919Z",
     "iopub.status.idle": "2023-09-27T13:15:37.306504Z",
     "shell.execute_reply": "2023-09-27T13:15:37.305332Z",
     "shell.execute_reply.started": "2023-09-27T13:15:36.407954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Import the RougeL metric from keras_nlp.metrics module\n",
    "import keras_nlp\n",
    "rouge_l = keras_nlp.metrics.RougeL()\n",
    "\n",
    "# Define a function to compute the RougeL metric\n",
    "def metric_fn(eval_predictions):\n",
    "    # Unpack the predictions and labels\n",
    "    predictions, labels = eval_predictions\n",
    "\n",
    "    # Decode the predicted tokens to text\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace masked label tokens\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode the label tokens to text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute the RougeL score\n",
    "    result = rouge_l(decoded_labels, decoded_predictions)\n",
    "\n",
    "    # We will print only the F1 score, you can use other aggregation metrics as well\n",
    "    result = {\"RougeL\": result[\"f1_score\"]}\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 7. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 7.1. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T16:12:50.211931Z",
     "iopub.status.busy": "2023-09-27T16:12:50.211154Z",
     "iopub.status.idle": "2023-09-27T16:12:50.684704Z",
     "shell.execute_reply": "2023-09-27T16:12:50.683339Z",
     "shell.execute_reply.started": "2023-09-27T16:12:50.211882Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "metric_callback = KerasMetricCallback(metric_fn,eval_dataset=generation_dataset, predict_with_generate=True)\n",
    "callbacks = [metric_callback]\n",
    "# For now we will use our test set as our validation_data\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://symbolize.stripped_domain/r/?trace=7af9b354c4f9,7af9b3486f8f,7af842acb0e8,7af842ac8692,7af842ac8b96,7af84387d09a,7af8333f5a7c,7af82d829614,7af82d82a1ea,7af8333ed5fb,7af82d823975,7af82d8265d3,7af82d80b148,7af8333fe243,7af8393b835a,7af84214ae24,7af71777ca22,7af71777cd13,7af717780615,7af9b377fb3d,100000000&map=aef7fe2e538f701f46d88df9ee3b51d79ec62b1e:7af84227b000-7af844256728,70f32832c9000a2f02b2bd353a9dfb35c743ca88:7af842017000-7af842264750,8f79f803f683427be94b1cfeea32716e6ef365e4:7af829915000-7af84186a830\n",
    "*** SIGTERM received by PID 15 (TID 15) on cpu 44 from PID 1; stack trace: ***\n",
    "PC: @     0x7af9b354c4f9  (unknown)  syscall\n",
    "    @     0x7af828c3605a       1152  (unknown)\n",
    "    @     0x7af9b3486f90  1872873984  (unknown)\n",
    "    @     0x7af842acb0e9        176  nsync::nsync_sem_wait_with_cancel_()\n",
    "    @     0x7af842ac8693        144  nsync::nsync_cv_wait_with_deadline_generic()\n",
    "    @     0x7af842ac8b97         32  nsync::nsync_cv_wait_with_deadline()\n",
    "    @     0x7af84387d09b        464  tensorflow::ProcessFunctionLibraryRuntime::RunSync()\n",
    "    @     0x7af8333f5a7d        368  tensorflow::KernelAndDeviceFunc::Run()\n",
    "    @     0x7af82d829615        480  tensorflow::EagerKernelExecute()\n",
    "    @     0x7af82d82a1eb        528  tensorflow::ExecuteNode::Run()\n",
    "    @     0x7af8333ed5fc         96  tensorflow::EagerExecutor::SyncExecute()\n",
    "    @     0x7af82d823976       1152  tensorflow::(anonymous namespace)::EagerLocalExecute()\n",
    "    @     0x7af82d8265d4       1136  tensorflow::EagerExecute()\n",
    "    @     0x7af82d80b149        208  tensorflow::EagerOperation::Execute()\n",
    "    @     0x7af8333fe244        208  tensorflow::CustomDeviceOpHandler::Execute()\n",
    "    @     0x7af8393b835b         80  TFE_Execute\n",
    "    @     0x7af84214ae25       1440  TFE_Py_ExecuteCancelable()\n",
    "    @     0x7af71777ca23        208  tensorflow::TFE_Py_ExecuteCancelable_wrapper()\n",
    "    @     0x7af71777cd14        208  pybind11::cpp_function::initialize<>()::{lambda()#3}::_FUN()\n",
    "    @     0x7af717780616        768  pybind11::cpp_function::dispatcher()\n",
    "    @     0x7af9b377fb3e  (unknown)  cfunction_call_varargs\n",
    "    @        0x100000001  (unknown)  (unknown)\n",
    "https://symbolize.stripped_domain/r/?trace=7af9b354c4f9,7af828c36059,7af9b3486f8f,7af842acb0e8,7af842ac8692,7af842ac8b96,7af84387d09a,7af8333f5a7c,7af82d829614,7af82d82a1ea,7af8333ed5fb,7af82d823975,7af82d8265d3,7af82d80b148,7af8333fe243,7af8393b835a,7af84214ae24,7af71777ca22,7af71777cd13,7af717780615,7af9b377fb3d,100000000&map=aef7fe2e538f701f46d88df9ee3b51d79ec62b1e:7af84227b000-7af844256728,70f32832c9000a2f02b2bd353a9dfb35c743ca88:7af842017000-7af842264750,8f79f803f683427be94b1cfeea32716e6ef365e4:7af829915000-7af84186a830,1278088d049ad36cb636fbbc76303cb3:7af81d600000-7af828e4d7c0 \n",
    "E0927 15:10:54.481297      15 coredump_hook.cc:360] RAW: Remote crash gathering disabled for SIGTERM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Sir i have tried using TPU's GPU's and normal CPU's for the computation purpose but since the dataset is too large everytime it was either thorwing oom error or it used to get disconnected. Despite of trying for 2 days i was unable to train the model however upon reaching the college i will use the GPU lab and get this model trained over there meanwhile i will be trying to train the model on google colab and kaggle as well if it get's trained i will update the file in the github where this is uploaded. Thanks sir for understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 8. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 8.1. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T16:12:50.685490Z",
     "iopub.status.idle": "2023-09-27T16:12:50.685868Z",
     "shell.execute_reply": "2023-09-27T16:12:50.685690Z",
     "shell.execute_reply.started": "2023-09-27T16:12:50.685672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the summarization pipeline from transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a summarization pipeline with the specified model and tokenizer\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "\n",
    "# Print a separator line\n",
    "print(\"-\" * 1000)\n",
    "\n",
    "# Print the original article content\n",
    "print(\"________Original Article__________\")\n",
    "print(datasets['test'][0]['document'])\n",
    "\n",
    "# Print a separator line\n",
    "print(\"-\" * 1000)\n",
    "\n",
    "# Print the original summary\n",
    "print(\"________Original Summary__________\")\n",
    "print(datasets['test'][0]['summary'])\n",
    "\n",
    "# Print a separator line\n",
    "print(\"-\" * 1000)\n",
    "\n",
    "# Generate and print the predicted summary\n",
    "summarizer(datasets[\"test\"][0][\"document\"], min_length=MIN_TARGET_LENGTH, max_length=MAX_TARGET_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 9. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T16:12:50.687179Z",
     "iopub.status.idle": "2023-09-27T16:12:50.687580Z",
     "shell.execute_reply": "2023-09-27T16:12:50.687376Z",
     "shell.execute_reply.started": "2023-09-27T16:12:50.687358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save_pretrained(\"T5CustomSummarizer\",from_pt=True)\n",
    "tokenizer.save_pretrained(\"T5CustomSumTokenizer\",from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
